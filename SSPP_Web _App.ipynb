{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ENGG1200 Data Analysis</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"analytics.jpg\" width=\"500\" height=\"100\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf5e277144c46b2825696c4ef624e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Statistics', layout=Layout(width='175px'), style=Buttoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import libraries\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "import nltk as nl\n",
    "nl.download('punkt',quiet=True)\n",
    "nl.download('stopwords',quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import collections\n",
    "from nltk.text import Text\n",
    "import re \n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from operator import itemgetter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from ipywidgets import Button, HBox, VBox, Layout, Box\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis.sklearn\n",
    "from pivottablejs import pivot_ui\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "words = ['Statistics','Visualization', 'WordClouds','Topic Modelling','Sentiment Analysis','Reflections','More Visualization']\n",
    "items = [Button(description=w,button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    layout=Layout(width='175px')) for w in words]\n",
    "display(HBox([item for item in items]))\n",
    "np.warnings.filterwarnings('ignore')\n",
    "def stats(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    #info about original and cleaned data\n",
    "    print(\"\\nThere are {} observations and {} features in original dataset and {} observations and {} features in cleaned dataset. \\n\".format(df.shape[0],df.shape[1],df_cleaned.shape[0],df_cleaned.shape[1])) \n",
    "    print(\"\\nA peek into originl data\\n\")\n",
    "    display(df.head())\n",
    "    print(\"\\n\\nA peek into cleaned data\\n\")\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    display(df_cleaned.head())\n",
    "    print(\"\\n\\nGrouped by Gender (after cleaning)\\n\")\n",
    "    #group by gender\n",
    "    gender = df_cleaned.groupby(\"Gender\")\n",
    "    display(gender.describe().head())\n",
    "    #group by international students\n",
    "    print(\"\\n\\nGrouped by International-Domestic students (after cleaning)\\n\")\n",
    "    international = df_cleaned.groupby(\"International\")\n",
    "    display(international.describe().head())\n",
    "items[0].on_click(stats)\n",
    "\n",
    "def visual(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    #cleaning data-removing NaN values\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    #extracting reviews \n",
    "    all_reviews=df_cleaned['Answer/Reflection']\n",
    "    print(\"\\n\\n\")\n",
    "    #entire cohort group by gender\n",
    "    entire_gender = df.groupby(\"Gender\")\n",
    "    #plot graph of entire gender distribution\n",
    "    gender_labels = 'Male', 'Female'\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(221)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    entire_gender.size().sort_values(ascending=False).plot.pie(labels = gender_labels, autopct='%1.1f%%')\n",
    "    plt.ylabel('')\n",
    "    plt.title('Gender Distribution in ENGG1200')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    #entire cohort group by international/domestic\n",
    "    entire_international = df.groupby(\"International\")\n",
    "    #plot graph of int/dom distribution\n",
    "    plt.subplot(222)\n",
    "    international_domestic_labels = 'Domestic', 'International'\n",
    "    entire_international.size().sort_values(ascending=False).plot.pie(labels = international_domestic_labels, autopct='%1.1f%%')\n",
    "    plt.ylabel('')\n",
    "    plt.title('International-Domestic Students Distribution in ENGG1200')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #group by gender\n",
    "    gender = df_cleaned.groupby(\"Gender\")\n",
    "    #plot graph of gender vs reviews\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(223)\n",
    "    gender.size().sort_values(ascending=False).plot.bar()\n",
    "    plt.xticks(rotation=50)\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(np.arange(2), ('Male', 'Female'))\n",
    "    plt.ylabel(\"Number of reflections\")\n",
    "    #might need to come up with a better graph title\n",
    "    plt.title('Valid Reflections Grouped By Gender')\n",
    "    \n",
    "    \n",
    "    #valid reflections group by international students\n",
    "    international = df_cleaned.groupby(\"International\")\n",
    "    #plot graph of international vs reviews\n",
    "    plt.subplot(224)\n",
    "    international.size().sort_values(ascending=False).plot.bar()\n",
    "    plt.xticks(rotation=50)\n",
    "    plt.xticks(np.arange(2), ('Domestic', 'International'))\n",
    "    plt.ylabel(\"Number of reflections\")\n",
    "    plt.xlabel('')\n",
    "    #might need to come up with a better graph title\n",
    "    plt.title('Valid Reflections Grouped By International-Domestic Students')\n",
    "    plt.subplots_adjust(bottom=0.1, right=1.5, top=0.9)\n",
    "    plt.show()\n",
    "    \n",
    "    #count\n",
    "    male_count = df.loc[df['Gender'] == 'M'].shape[0]\n",
    "    female_count = df.loc[df['Gender'] == 'F'].shape[0]\n",
    "    domestic_count = df.loc[df['International'] == 'N'].shape[0]\n",
    "    international_count = df.loc[df['International'] == 'Y'].shape[0]\n",
    "    male_valid_count = df_cleaned.loc[df_cleaned['Gender'] == 'M'].shape[0]\n",
    "    female_valid_count = df_cleaned.loc[df_cleaned['Gender'] == 'F'].shape[0]\n",
    "    domestic_valid_count = df_cleaned.loc[df_cleaned['International'] == 'N'].shape[0]\n",
    "    international_valid_count = df_cleaned.loc[df_cleaned['International'] == 'Y'].shape[0]\n",
    "\n",
    "    #text\n",
    "    print(\"\\nThere are {} students in total.\\nMale: {} Female: {}.\\nDomestic: {} International: {}.\\n\".format(df.shape[0],male_count, female_count, domestic_count, international_count))\n",
    "    print(\"\\nThere are {} valid reflections (reflections in cleaned data) in total.\\nMale: {} Female: {}.\\nDomestic: {} International: {}.\\n\".format(df_cleaned.shape[0],male_valid_count, female_valid_count, domestic_valid_count, international_valid_count))\n",
    "\n",
    "\n",
    "\n",
    "items[1].on_click(visual) \n",
    "\n",
    "def wc(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    print(\"\\nPlease wait for the output to be generated.This may take a while.\\n\\n\")\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    #cleaning data-removing NaN values\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    #extracting reviews \n",
    "    all_reviews=df_cleaned['Answer/Reflection']\n",
    "    #tokenization,removing stopwords, punctuation and stemming\n",
    "    all_ans=\"\"\n",
    "    for review in all_reviews:\n",
    "        all_ans=all_ans+review+\"\\n\"\n",
    "    all_ans= all_ans.replace(\"'\", \"\")\n",
    "    tokens=word_tokenize(all_ans)\n",
    "    tokens=[w.lower() for w in tokens]\n",
    "    text = nl.Text(tokens)\n",
    "    token_words=[word for word in tokens if word.isalpha()]\n",
    "    stopword=stopwords.words('english')\n",
    "    stopword.append('dont')\n",
    "    stopword.append('didnt')\n",
    "    stopword.append('doesnt')\n",
    "    stopword.append('cant')\n",
    "    stopword.append('couldnt')\n",
    "    stopword.append('couldve')\n",
    "    stopword.append('im')\n",
    "    stopword.append('ive')\n",
    "    stopword.append('isnt')\n",
    "    stopword.append('theres')\n",
    "    stopword.append('wasnt')\n",
    "    stopword.append('wouldnt')\n",
    "    stopword.append('a')\n",
    "    stopword.append('also')\n",
    "    token_words=[w for w in token_words if not w in stopword]\n",
    "    porter =PorterStemmer()\n",
    "    token_stemmed=[porter.stem(w) for w in token_words]\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    #creating worlcloud\n",
    "    cloudstring=(\" \").join(token_words)\n",
    "    wordcloud = WordCloud(max_font_size=50,max_words=100, background_color=\"black\").generate(cloudstring)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    ax = plt.subplot(221)\n",
    "    # plot wordcloud in matplotlib\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"WordCloud generated from reflections\")\n",
    "    plt.grid(True)\n",
    "    #plotting bi-gram cloud\n",
    "    # setup and score the bigrams using the raw frequency.\n",
    "    finder = BigramCollocationFinder.from_words(token_words)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    "    word_dict = {} \n",
    "    listLen = len(scoredList)\n",
    "    for i in range(listLen):\n",
    "        word_dict['_'.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "    wordCloud = WordCloud(max_font_size=50, max_words=100, background_color=\"black\")\n",
    "    plt.subplot(222) \n",
    "    wordCloud.generate_from_frequencies(word_dict)\n",
    "\n",
    "    plt.title('Most frequently occurring bigrams connected with an underscore_')\n",
    "    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    #plotting frequency distribution\n",
    "    plt.figure(figsize=(25,5))\n",
    "    ax = plt.subplot(121)\n",
    "    freqdist = nl.FreqDist(token_words)\n",
    "    plt.subplot(121) \n",
    "    plt.title(\"Frequency Distribution of token words\")\n",
    "    freqdist.plot(50)\n",
    "    \n",
    "    #dispertion plot\n",
    "    plt.figure(figsize=(25,5))\n",
    "    ax = plt.subplot(122) \n",
    "    text.dispersion_plot([str(w) for w, f in freqdist.most_common(20)])\n",
    "\n",
    "    \n",
    "items[2].on_click(wc)    \n",
    "\n",
    "\n",
    "def tm(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    print(\"\\nPlease wait for the output to be generated.This may take a while.\\n\\n\")\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    #cleaning data-removing NaN values\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    NUM_TOPICS = 10\n",
    "    data=df_cleaned['Answer/Reflection'] \n",
    "    vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "                                 stop_words='english', lowercase=True, \n",
    "                                 token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "    data_vectorized = vectorizer.fit_transform(data)\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation Model\n",
    "    lda_model = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "    lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "    display(panel)\n",
    "\n",
    "items[3].on_click(tm)\n",
    "\n",
    "def sa(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    print(\"\\nPlease wait for the output to be generated.This may take a while.\\n\\n\")\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    #cleaning data-removing NaN values\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    all_reviews=df_cleaned['Answer/Reflection']\n",
    "    #sentiment analysis\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    international_students = df_cleaned.loc[df_cleaned['International'] == 'Y']\n",
    "    international_reflections = international_students['Answer/Reflection']\n",
    "    domestic_students = df_cleaned.loc[df_cleaned['International'] == 'N']\n",
    "    domestic_reflections = domestic_students['Answer/Reflection']\n",
    "    pos_entire=[]\n",
    "    neg_entire=[]\n",
    "    neutral_entire=[]\n",
    "\n",
    "    pos_int=[]\n",
    "    neg_int=[]\n",
    "    neutral_int=[]\n",
    "\n",
    "    pos_dom=[]\n",
    "    neg_dom=[]\n",
    "    neutral_dom=[]\n",
    "    for review in all_reviews:\n",
    "        scores = analyser.polarity_scores(review)\n",
    "        if scores['compound']<=-0.5:\n",
    "            neg_entire.append(review)\n",
    "        if scores['compound']>=0.5:\n",
    "            pos_entire.append(review)\n",
    "        if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "            neutral_entire.append(review)\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))        \n",
    "    type_length=[len(pos_entire),len(neutral_entire),len(neg_entire)]\n",
    "    sent_type=['positive','neutral','negative']\n",
    "    plt.pie(type_length, labels=sent_type, startangle=90, autopct='%.1f%%')\n",
    "    plt.title('Sentiment distribution for entire cohort')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    for review in international_reflections:\n",
    "        scores = analyser.polarity_scores(review)\n",
    "        if scores['compound']<=-0.5:\n",
    "            neg_int.append(review)\n",
    "        if scores['compound']>=0.5:\n",
    "            pos_int.append(review)\n",
    "        if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "            neutral_int.append(review)\n",
    "            \n",
    "            \n",
    "    type_length=[len(pos_int),len(neutral_int),len(neg_int)]\n",
    "    sent_type=['positive','neutral','negative']\n",
    "    plt.pie(type_length, labels=sent_type, startangle=90, autopct='%.1f%%')\n",
    "    plt.title('Sentiment distribution for international students cohort')\n",
    "    plt.show()\n",
    "\n",
    "    for review in domestic_reflections:\n",
    "        scores = analyser.polarity_scores(review)\n",
    "        if scores['compound']<=-0.5:\n",
    "            neg_dom.append(review)\n",
    "        if scores['compound']>=0.5:\n",
    "            pos_dom.append(review)\n",
    "        if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "            neutral_dom.append(review)\n",
    "    \n",
    "    type_length=[len(pos_dom),len(neutral_dom),len(neg_dom)]\n",
    "    sent_type=['positive','neutral','negative']\n",
    "    plt.pie(type_length, labels=sent_type, startangle=90, autopct='%.1f%%')\n",
    "    plt.title('Sentiment distribution for domestic students cohort')\n",
    "    plt.show()\n",
    "\n",
    "    bars1 = [len(pos_entire), len(pos_dom), len(pos_int)]\n",
    "    bars2 = [len(neutral_entire), len(neutral_dom), len(neutral_int)]\n",
    "    bars3 = [len(neg_entire), len(neg_dom), len(neg_int)]\n",
    "    sent_type=['positive','neutral','negative']\n",
    "\n",
    "    # set width of bar\n",
    "    barWidth = 0.25\n",
    "    # Set position of bar on X axis\n",
    "    r1 = np.arange(len(bars1))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.bar(r1, bars1, width=barWidth, edgecolor='white', label='Positive')\n",
    "    plt.bar(r2, bars2, width=barWidth, edgecolor='white', label='Neutral')\n",
    "    plt.bar(r3, bars3, width=barWidth, edgecolor='white', label='Negative')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Group', fontweight='bold')\n",
    "    plt.xticks([r + barWidth for r in range(len(bars1))], ['Entire Cohort', 'Domestic Students', 'International Students'])\n",
    "\n",
    "\n",
    "    # Create legend & Show graphic\n",
    "    plt.legend()\n",
    "    plt.title('Sentiment Distributions')\n",
    "    plt.show()\n",
    "    \n",
    "items[4].on_click(sa)\n",
    "\n",
    "def ref(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    print(\"\\nPlease wait for the output to be generated.This may take a while.\\n\\n\")\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    #cleaning data-removing NaN values\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    all_reviews=df_cleaned['Answer/Reflection']\n",
    "    pos_entire=[]\n",
    "    neg_entire=[]\n",
    "    neutral_entire=[]\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    for review in all_reviews:\n",
    "        scores = analyser.polarity_scores(review)\n",
    "        if scores['compound']<=-0.5:\n",
    "            neg_entire.append(review)\n",
    "        if scores['compound']>=0.5:\n",
    "            pos_entire.append(review)\n",
    "        if scores['compound']>-0.5 and scores['compound']<0.5:\n",
    "            neutral_entire.append(review)\n",
    "    #sentiment analysis\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    bpos = widgets.Button(description=\"Next positive reflection\",button_style='success',layout=Layout(width='175px'))\n",
    "    bneg = widgets.Button(description=\"Next negative reflection\",button_style='danger',layout=Layout(width='175px'))\n",
    "    bneu = widgets.Button(description=\"Next neutral reflection\",button_style='info',layout=Layout(width='175px'))\n",
    "    display(HBox([bpos,bneu,bneg]))\n",
    "    \n",
    "    \n",
    "    def posref(b):\n",
    "        clear_output()\n",
    "        display(HBox([item for item in items]))\n",
    "        display(HBox([bpos,bneu,bneg]))\n",
    "        print(\"\\nPositive reflection\\n\")\n",
    "        display(pos_entire[0])\n",
    "        item_rem=pos_entire.pop(0)\n",
    "        pos_entire.append(item_rem)\n",
    "        \n",
    "        \n",
    "        \n",
    "    bpos.on_click(posref)\n",
    "    \n",
    "    def neuref(b):\n",
    "        clear_output()\n",
    "        display(HBox([item for item in items]))\n",
    "        display(HBox([bpos,bneu,bneg]))\n",
    "        print(\"\\nNeutral reflection\\n\")\n",
    "        display(neutral_entire[0])\n",
    "        item_rem=neutral_entire.pop(0)\n",
    "        neutral_entire.append(item_rem)\n",
    "        \n",
    "        \n",
    "        \n",
    "    bneu.on_click(neuref)\n",
    "    \n",
    "    def negref(b):\n",
    "        clear_output()\n",
    "        display(HBox([item for item in items]))\n",
    "        display(HBox([bpos,bneu,bneg]))\n",
    "        print(\"\\nNegative reflection\\n\")\n",
    "        display(neg_entire[0])\n",
    "        item_rem=neg_entire.pop(0)\n",
    "        neg_entire.append(item_rem)\n",
    "        \n",
    "        \n",
    "        \n",
    "    bneg.on_click(negref)\n",
    "    \n",
    "items[5].on_click(ref)  \n",
    "\n",
    "def mv(b):\n",
    "    clear_output()\n",
    "    display(HBox([item for item in items]))\n",
    "    #import dataset \n",
    "    file = r'engg.xls'\n",
    "    df = pd.read_excel(file)\n",
    "    #cleaning data-removing NaN values\n",
    "    df_cleaned=df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned['Answer/Reflection'] != '-No Answer-']\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "    display(pivot_ui(df_cleaned))\n",
    "    \n",
    "items[6].on_click(mv)\n",
    "np.warnings.filterwarnings('ignore')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
